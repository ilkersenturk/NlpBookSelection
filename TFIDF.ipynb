{
 "cells": [
  {
   "cell_type": "raw",
   "id": "2a1bacc7",
   "metadata": {},
   "source": [
    "TF-IDF (term frequency-inverse document frequency) is a statistical measure that evaluates how relevant a word is to a document in a collection of documents.\n",
    "\n",
    "This is done by multiplying two metrics: how many times a word appears in a document, and the inverse document frequency of the word across a set of documents.\n",
    "\n",
    "\n",
    "\n",
    "        1-)Compute Tfidf for a word in the document (takes word query, document , all documents)\n",
    "        \n",
    "            Tf  :Count the number of words / len(document)\n",
    "       \n",
    "            Idf : Number of the word occurance  / num of total words\n",
    "            \n",
    "        Convert to vector\n",
    "            \n",
    "            * [tfidf for eac word where words are in uniqe set of documents\n",
    "        3-) choose a query for document search\n",
    "            convet query to vector\n",
    "        \n",
    "        4-) Compute Cosine similarty between documents and query \n",
    "        higher one is the one\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a4c82334",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "#import the pickle documents\n",
    "document_1 = pd.read_pickle(\"document_1.pickle\")\n",
    "document_2 = pd.read_pickle(\"document_2.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "841e6f6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'martial peakw lin din fng journey martial peak lonely solitary long face adversity survive remain unyielding break continue journey strong high heaven pavilion test disciple harshest way prepare journey day lowly sweeper kai yang manage black book set road peak martial worldauthorsmomo artistsyear c'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document_1[:300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "26c3d4eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'chapter 0 magic cubein vast limitless expanse mist snow endless ice shard swirl wind collide violent maelstrom subzero temperature chill bone freeze cold temperature turn soul icehere snowfall realm myriad dimension realm god bleak hopeless land endless blinding white tundra year desolate snow bitte'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document_2[:300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fce7b3f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeTfIdf(word,sentence,documents):\n",
    "    \"\"\"\n",
    "    Calculates :\n",
    "    term frequency : tf ==>word count / total words in sentences\n",
    "    inverse term frequency : idf ==> log10(total_num_doc / num_doc has the word)\n",
    "    \n",
    "    Input\n",
    "    -----------\n",
    "    word :str a word  \n",
    "    \n",
    "    sentence: str sentence or document\n",
    "    -----------\n",
    "    \n",
    "    Return\n",
    "    -----------\n",
    "    score : float \n",
    "    \n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "    #counting the occurance of the word in current document / normalize\n",
    "    tf  = sentence.count(word) / float(len(sentence.split()))\n",
    "    # counting all documents / counting the document has the word\n",
    "    idf = np.log10(len(documents)/1 + sum([1 for doc in documents if word in doc]  ))\n",
    "  \n",
    "    score = round(tf * idf, 4)\n",
    "    return score\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4895d98c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0015"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents = [document_1,document_2]\n",
    "query =\"martial\"\n",
    "\n",
    "computeTfIdf(query, document_1, documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "44ed8186",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0092"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "computeTfIdf(query, document_2,documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "356af9a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def tfidf_Vector(document,documents):\n",
    "    \n",
    "    # all the unique words are in documents\n",
    "    vocabularies = set(documents[0].split())\n",
    "    for i in range(1, len(documents)-1):\n",
    "        #addin all the word to vocabularies\n",
    "        vocabularies.extends(documents[i].split())\n",
    "    \n",
    "    # getting only uniqe words \n",
    "    unique_words = set(vocabularies)\n",
    "    return [computeTfIdf(word,document, documents) for word in unique_words]\n",
    "\n",
    "\n",
    "tfidf_vector_1 = tfidf_Vector(document_1,documents)\n",
    "tfidf_vector_2 = tfidf_Vector(document_2,documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d5451b8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kai yang martial peak\n"
     ]
    }
   ],
   "source": [
    "# Which book i should be reading ?\n",
    "#  I mostly read manga such as Martial Peak, One Piece Naruto ...etc i will be copying some content as a query\n",
    "# and compaing their similarities with documents\n",
    "\n",
    "\n",
    "query =\"\"\"\n",
    "Kai Yang is the martial peak\n",
    "\"\"\" \n",
    "from helper_function import cleanText,removeStopWords,lemmatizeText\n",
    "\n",
    "query = lemmatizeText(removeStopWords(cleanText(query)))\n",
    "print(query)\n",
    "\n",
    "\n",
    "tfidf_query = tfidf_Vector(query, documents)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "264b4837",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosineSimilarity(vec1, vec2):\n",
    "    \n",
    "    import numpy as np\n",
    "    \n",
    "    product = np.dot(np.array(vec1), np.array(vec2))\n",
    "\n",
    "    norm_1 = np.linalg.norm(vec1)\n",
    "    norm_2 = np.linalg.norm(vec2)\n",
    "    \n",
    "    return product /(norm_1 * norm_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ba76e2a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "score_1 = cosineSimilarity(tfidf_vector_1, tfidf_query)\n",
    "score_2 = cosineSimilarity(tfidf_vector_2, tfidf_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8aa8701c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4630653171828923 0.17535463906982704\n"
     ]
    }
   ],
   "source": [
    "print(score_1,score_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9c806018",
   "metadata": {},
   "outputs": [],
   "source": [
    "# since first score_1> greater we will simple recommend first document\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4586a086",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
